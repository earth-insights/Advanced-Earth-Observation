# Advanced LLM

- RSGPT: A Remote Sensing Vision Language Model and Benchmark. arXiv'2023. [[Paper](https://arxiv.org/abs/2307.15266) | [Code](https://github.com/Lavender105/RSGPT)]
- GeoChat: Grounded Large Vision-Language Model for Remote Sensing. CVPR'2024. [[Paper](https://arxiv.org/abs/2311.15826) | [Code](https://github.com/mbzuai-oryx/GeoChat)]
- Charting New Territories: Exploring the Geographic and Geospatial Capabilities of Multimodal LLMs. CVPRW'2024. [[paper](https://arxiv.org/abs/2311.14656) | [code](https://github.com/jonathan-roberts1/charting-new-territories)]
- Good at captioning, bad at counting: Benchmarking GPT-4V on Earth observation data. CVPRW'2024. [[Paper](https://arxiv.org/abs/2401.17600) | [Code](https://vleo.danielz.ch/)]
- EarthVQA: Towards Queryable Earth via Relational Reasoning-Based Remote Sensing Visual Question Answering. AAAI'2024. [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/28357) | [Code](https://github.com/Junjue-Wang/EarthVQA)]
- Earthgpt: A universal multi-modal large language model for multi-sensor image comprehension in remotesensing domain. TGRS'2024. [[Paper](https://arxiv.org/abs/2401.16822) | [Code](https://github.com/wivizhang/EarthGPT)]
- EarthMarker: Visual Prompt Learning for Region-level and Point-level Remote Sensing Imagery Comprehension. TGRS'2024. [[paper](https://arxiv.org/abs/2407.13596) | [code](https://github.com/wivizhang/EarthMarker)]
- RingMoGPT: A Unified Remote Sensing Foundation Model for Vision, Language, and grounded tasks. TGRS'2024. [[paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10777289)]
- LHRS-Bot: Empowering remote sensing with vgi-enhanced large multimodal language model. ECCV'2024. [[Paper](https://arxiv.org/abs/2402.02544) | [Code](https://github.com/NJU-LHRS/LHRS-Bot)]
- VRSBench: A Versatile Vision-Language Benchmark Dataset for Remote Sensing Image Understanding. arXiv'2024. [[Paper](https://arxiv.org/abs/2406.12384) | [Code](https://vrsbench.github.io/)]
- Aquila: A Hierarchically Aligned Visual-Language Model for Enhanced Remote Sensing Image Comprehension. arXiv'2024. [[paper](https://arxiv.org/pdf/2411.06074)]
- RSUniVLM: A Unified Vision Language Model for Remote Sensing via Granularity-oriented Mixture of Experts. arXiv'2024. [[paper](https://arxiv.org/abs/2412.05679) | [code](https://github.com/xuliu-cyber/RSUniVLM)]
- UniRS: Unifying Multi-temporal Remote Sensing Tasks through Vision Language Models. arXiv'2024. [[paper](https://arxiv.org/abs/2412.20742)]
- SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model. ISPRS P&RS'2025. [[paper](https://arxiv.org/abs/2401.09712) | [code](https://github.com/ZhanYang-nwpu/SkyEyeGPT)]
- LHRS-Bot-Nova: Improved Multimodal Large Language Model for Remote Sensing Vision-Language Interpretation. ISPRS P&RS'2025. [[paper](https://arxiv.org/pdf/2411.09301) | [code](https://github.com/NJU-LHRS/LHRS-Bot)]
- SkySenseGPT: A Fine-Grained Instruction Tuning Dataset and Model for Remote Sensing Vision-Language Understanding. arXiv'2025. [[Paper](https://arxiv.org/abs/2406.10100) | [Code](https://github.com/Luo-Z13/SkySenseGPT)]
- Falcon: A Remote Sensing Vision-Language Foundation Model. arXiv'2025. [[Paper](https://arxiv.org/abs/2503.11070) | [Code](https://github.com/TianHuiLab/Falcon)]
- TEOChat: A Large Vision-Language Assistant for Temporal Earth Observation Data. ICLR'2025. [[Paper](https://arxiv.org/abs/2410.06234) | [Code](https://github.com/ermongroup/TEOChat)]
- VHM: Versatile and Honest Vision Language Model for Remote Sensing Image Analysis. AAAI'2025. [[Paper](https://arxiv.org/abs/2403.20213v4) | [Code](https://github.com/opendatalab/VHM)]
- XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery? CVPR'2025. [[Paper](https://arxiv.org/abs/2503.23771) | [Code](https://xlrs-bench.github.io/)]
- EarthDial: Turning Multi-sensory Earth Observations to Interactive Dialogues. arXiv'2024. [[paper](https://openaccess.thecvf.com/content/CVPR2025/html/Soni_EarthDial_Turning_Multi-sensory_Earth_Observations_to_Interactive_Dialogues_CVPR_2025_paper.html) | [Code](https://github.com/hiyamdebary/EarthDial)]
- REO-VLM: Transforming VLM to Meet Regression Challenges in Earth Observation. arXiv'2024. [[paper](https://arxiv.org/abs/2412.16583)]
- AllSpark: A Multimodal Spatiotemporal General Intelligence Model With Ten Modalities via Language as a Reference Framework. TGRS'2025. [[Paper](https://ieeexplore.ieee.org/abstract/document/10830573/) | [Code](https://github.com/GeoX-Lab/AllSpark)]
- Towards Efficient Benchmarking of Foundation Models in Remote Sensing: A Capabilities Encoding Approach. CVPRW'2025. [[Paper](https://openaccess.thecvf.com/content/CVPR2025W/MORSE/html/Adorni_Towards_Efficient_Benchmarking_of_Foundation_Models_in_Remote_Sensing_A_CVPRW_2025_paper.html) | [Code](https://github.com/pierreadorni/capabilities-encoding)]
- UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding. ICCV'2025. [[Paper](https://arxiv.org/abs/2506.23219) | [Code](https://github.com/tsinghua-fib-lab/UrbanLLaVA)]
- When Large Vision-Language Model Meets Large Remote Sensing Imagery: Coarse-to-Fine Text-Guided Token Pruning. ICCV'2025. [[Paper](https://arxiv.org/abs/2503.07588) | [Code](https://github.com/VisionXLab/LRS-VQA)]
- GEOBench-VLM: Benchmarking Vision-Language Models for Geospatial Tasks. ICCV'2025. [[Paper](https://arxiv.org/abs/2411.19325) | [Code](https://github.com/The-AI-Alliance/GEO-Bench-VLM)]
- GeoPix: Multi-Modal Large Language Model for Pixel-level Image Understanding in Remote Sensing. arXiv'2025. [[paper](https://arxiv.org/abs/2501.06828) | [Code](https://github.com/Norman-Ou/GeoPix)]
- Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models. arXiv'2025. [[paper](https://arxiv.org/abs/2503.00743)]
- When Large Vision-Language Model Meets Large Remote Sensing Imagery: Coarse-to-Fine Text-Guided Token Pruning. arXiv'2025. [[paper](https://arxiv.org/abs/2503.07588) | [code](https://github.com/VisionXLab/LRS-VQA)]
- EagleVision: Object-level Attribute Multimodal LLM for Remote Sensing. arXiv'2025. [[paper](https://arxiv.org/abs/2503.23330) | [code](https://github.com/XiangTodayEatsWhat/EagleVision)]
- OmniGeo: Towards a Multimodal Large Language Models for Geospatial Artificial Intelligence. arXiv'2025. [[paper](https://arxiv.org/abs/2503.16326)]
- Can Large Multimodal Models Understand Agricultural Scenes? Benchmarking with AgroMind. arXiv'2025. [[Paper](https://arxiv.org/abs/2505.12207) | [Code](https://github.com/rssysu/AgroMind)]
- Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling. arXiv'2025. [[Paper](https://arxiv.org/abs/2506.21863)]
- Landsat-Bench: Datasets and Benchmarks for Landsat Foundation Models. arXiv'2025. [[Paper](https://arxiv.org/abs/2506.08780) | [Code](https://github.com/isaaccorley/landsatbench)]
- Few-Shot Vision-Language Reasoning for Satellite Imagery via Verifiable Rewards. arXiv'2025. [[Paper](https://arxiv.org/abs/2507.21745)]
- GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution. NeurlPS'2025. [[Paper](https://arxiv.org/abs/2505.21375) | [Code](https://github.com/MiliLab/GeoLLaVA-8K)]
- GeoVLM-R1: Reinforcement Fine-Tuning for Improved Remote Sensing Reasoning. arXiv'2025. [[Paper](https://arxiv.org/abs/2509.25026) | [Code](https://mustansarfiaz.github.io/GeoVLM-R1/)]
